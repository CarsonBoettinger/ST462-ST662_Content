---
title: "ST462_ST662_Lab1"
subtitle: "Multiple Regression Analysis"
output:
  html_document: default
  pdf_document:
    number_sections: yes
---

\mainmatter

# Review of Multiple Linear Regression {#ch-MLRreview}

## Learning Objectives
  After finishing this chapter, you should be able to:

- Identify cases where linear least squares regression (LLSR) assumptions are violated.
- Generate exploratory data analysis (EDA) plots and summary statistics.
- Use residual diagnostics to examine LLSR assumptions.
- Interpret parameters and associated tests and intervals from multiple regression models.
- Understand the basic ideas behind bootstrapped confidence intervals.

## Installing and Loading the packages

IMPORTANT: If installing the car package, you see a pop-up window with a message saying "Do you want to install from sources the package which needs compilation?", then please select "NO" and proceed for installation.

```{r load_packages1, message = FALSE}
# Packages required for Chapter 1

if (!require(gridExtra)) install.packages('gridExtra', dependencies = TRUE) 
library(gridExtra)
if (!require(knitr)) install.packages('knitr', dependencies = TRUE) 
library(knitr)
if (!require(GGally)) install.packages('GGally', dependencies = TRUE)
library(GGally)
if (!require(kableExtra)) install.packages('kableExtra', dependencies = TRUE) 
library(kableExtra)
if (!require(jtools)) install.packages('jtools', dependencies = TRUE) 
library(jtools)
if (!require(rsample)) install.packages('rsample', dependencies = TRUE)
library(rsample)
if (!require(broom)) install.packages('broom', dependencies = TRUE) 
library(broom)
if (!require(tidyverse)) install.packages('tidyverse', dependencies = TRUE)
library(tidyverse)
```

```{r include=FALSE}
if(knitr::is_html_output()){options(knitr.table.format = "html")} else {options(knitr.table.format = "latex")}
```

### Case Study: Kentucky Derby {#cs:derby}

The Kentucky Derby is a 1.25-mile horse race held annually at the Churchill Downs race track in Louisville, Kentucky.  Our data set `derbyplus.csv` contains the `year` of the race, the winning horse (`winner`), the `condition` of the track, the average `speed` (in feet per second) of the winner, and the number of `starters` (field size, or horses who raced) for the years 1896-2017 [@KentuckyDerby]. The track `condition` has been grouped into three categories: fast, good (which includes the official designations “good” and “dusty”), and slow (which includes the designations “slow”, “heavy”, “muddy”, and “sloppy”).  We would like to use least squares linear regression techniques to model the speed of the winning horse as a function of track condition, field size, and trends over time.  

## Initial Exploratory Analyses {#explorech1}
### Part 1: Data Organization

**Question 1.1:** Import the data and show the first six and last six rows from the data set.
```{r}
derby.df <- read.csv("/Users/boet9790/Downloads/derbyplus.csv")
head(derby.df)
tail(derby.df)
dim(derby.df)
table(derby.df)
```

Note in certain cases, we created new variables from existing ones:

- `fast` is an **indicator variable**, \index{indicator variable} taking the value 1 for races run on fast tracks, and 0 for races run under other conditions,
- `good` is another indicator variable, taking the value 1 for races run under good conditions, and 0 for races run under other conditions,
- `yearnew` is a **centered variable**, \index{centered variable} where we measure the number of years since 1896, and
- `fastfactor` replaces `fast` = 0 with the description "not fast", and `fast` = 1 with the description "fast".  Changing a numeric categorical variable to descriptive phrases can make plot legends more meaningful.

```{r introtable1,echo=FALSE, warning=FALSE}
derby.df <- derby.df %>%
  mutate( fast = ifelse(condition=="fast",1,0), 
          good = ifelse(condition=="good",1,0),
          yearnew = year - 1896,
          fastfactor = ifelse(fast == 0, "not fast", "fast"))
table1 <- derby.df %>%
  filter(row_number() < 6 | row_number() > 117)
kable(table1, booktabs=T,caption="The first five and the last five observations from the Kentucky Derby case study.") %>%
  kable_styling(latex_options = "scale_down")
```

### Univariate Summaries

With any statistical analysis, our first task is to explore the data, examining distributions of individual responses and predictors using graphical and numerical summaries, and beginning to discover relationships between variables.  This should *always* be done *before* any model fitting!  We must understand our data thoroughly before doing anything else.  

First, we will examine the response variable and each potential covariate individually.  Continuous variables can be summarized using histograms and statistics indicating center and spread; categorical variables can be summarized with tables and possibly bar charts.  

```{r twohist, fig.align = "center", out.width="90%", fig.cap = 'Histograms of key continuous variables.  Plot (a) shows winning speeds, while plot (b) shows the number of starters.', echo=FALSE, message=FALSE}
# EDA graphs
speed_hist <- ggplot(data = derby.df, aes(x = speed)) + 
  geom_histogram(binwidth = 0.5, fill = "white",
                 color = "black") + 
  xlab("Winning speed (ft/s)") + ylab("Frequency") + labs(title="(a)")
starters_hist <- ggplot(data = derby.df, aes(x = starters)) + 
  geom_histogram(binwidth = 3, fill = "white",
                 color = "black") + 
  xlab("Number of starters") + ylab("Frequency") + labs(title="(b)")
grid.arrange(speed_hist, starters_hist, ncol = 2)
```

**Question 1.2:** Comment on Figure \@ref(fig:twohist)(a) and (b): 
**Answer:**



**Question 1.3:** Draw a bar graph for the categorical explanatory variable _track_ _condition_, and report the frequencies or percent frequencies. 

```{r}
ggplot(data=derby.df, aes(x=condition))+
  geom_bar(stat="count") 
ggplot(data=derby.df,aes(x=condition))+
  geom_bar(aes(y = after_stat(prop), group = 1))
table(derby.df$condition)
table(derby.df$condition)/sum(table(derby.df$condition))
```

### Part 2: Bivariate Summaries

The next step in an initial exploratory analysis is the examination of numerical and graphical summaries of relationships between model covariates and responses.  Figure \@ref(fig:bivariate) is densely packed with illustrations of bivariate relationships.  The relationship between two continuous variables is depicted with scatterplots below the diagonal and correlation coefficients above the diagonal.  

```{r bivariate, fig.align = "center", out.width = "90%", fig.cap = 'Relationships between pairs of variables in the Kentucky Derby data set.', echo=FALSE, warning=FALSE, message = FALSE}
gg <- ggpairs(data = derby.df, 
              columns = c("condition", "year", "starters", "speed"))
gg[4,1] <- gg[4,1] + geom_histogram( binwidth = 0.75)
gg[2,1] <- gg[2,1] + geom_histogram( binwidth = 20)
gg[3,1] <- gg[3,1] + geom_histogram( binwidth = 3)
gg
```

By using shape or color or other attributes, we can incorporate the effect of a third or even fourth variable into the scatterplots of Figure  \@ref(fig:bivariate).  For example, in the **coded scatterplot** \index{coded scatterplot} of Figure \@ref(fig:codeds) we see that speeds are generally faster under fast conditions, but the rate of increasing speed over time is greater under good or slow conditions.  

```{r codeds, fig.align = "center", out.width = "90%", fig.cap = 'Linear trends in winning speeds over time, presented separately for fast conditions vs. good or slow conditions.', echo=FALSE, warning=FALSE, message=FALSE}
# Coded scatterplot
ggplot(derby.df, aes(x = year, y = speed, colour = fastfactor)) +
  geom_point(aes(shape = fastfactor)) +
  geom_smooth(aes(linetype = fastfactor), method = lm, se = FALSE)
```

Of course, any graphical analysis is exploratory, and any notable trends at this stage should be checked through formal modeling.  At this point, a statistician begins to ask familiar questions such as:

- are winning speeds increasing in a linear fashion?
- does the rate of increase in winning speed depend on track condition or number of starters?
- after accounting for other explanatory variables, is greater field size (number of starters) associated with faster winning speeds (because more horses in the field means a greater chance one horse will run a very fast time) or slower winning speeds (because horses are more likely to bump into each other or crowd each others' attempts to run at full gait)?
- are any of these associations statistically significant?
- how well can we predict the winning speed in the Kentucky Derby?

As you might expect, answers to these questions will arise from proper consideration of variability and properly identified statistical models.  

## Part 3: Multiple Linear Regression Modeling {#multreg}
### Question 3.1 Simple Linear Regression with a Continuous Predictor {#SLRcontinuous}

Fit a simple linear regression of modeling the winning speed as a function of time; i.e., have winning speeds increased at a constant rate since 1896?  For this initial model, let $Y_{i}$ be the speed of the winning horse in year $i$.  **Extract the coefficient estimates, R square and Residual Standard Error.** Then, we might consider Model 1: 

\begin{equation}
 Y_{i}=\beta_{0}+\beta_{1}\textrm{Year}_{i}+\epsilon_{i} \quad \textrm{where} \quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2).
(\#eq:model1)
\end{equation}


```{r model11, comment=NA}
model1 <- lm(speed ~ year, data = derby.df)
summary(model1)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model1))
cat(" R squared = ", summary(model1)$r.squared, "\n", 
    "Residual standard error = ", summary(model1)$sigma)
```
### Question 3.2 Centering variable _Year_

You may have noticed in Model 1 that the intercept has little meaning in context, since it estimates a winning speed in Year 0, when the first Kentucky Derby run at the current distance (1.25 miles) was in 1896.  One way to create more meaningful parameters is through **centering**. \index{centered variable}  In this case, we could create a centered year variable by subtracting 1896 from each year for Model 2:

\begin{equation*}
\begin{split}
Y_{i}=\beta_{0}+\beta_{1}\textrm{Yearnew}_{i}+\epsilon_{i}\quad &\textrm{where} \quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2) \\
    &\textrm{and} \quad \textrm{Yearnew}=\textrm{Year}-1896.
\end{split}
\end{equation*}

Note that the only thing that changes from Model 1 to Model 2 is the estimated intercept; $\hat{\beta}_{1}$, $R^2$, and $\hat{\sigma}$ all remain exactly the same.  Now $\hat{\beta}_{0}$ tells us that the estimated winning speed in 1896 is 51.59 ft/s, but estimates of the linear rate of improvement or the variability explained by the model remain the same.  As Figure \@ref(fig:center) shows, centering year has the effect of shifting the y-axis from year 0 to year 1896, but nothing else changes. 

```{r model2, comment=NA}
model2 <- lm(speed ~ yearnew, data = derby.df)
summary(model2)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model2))
cat(" R squared = ", summary(model2)$r.squared, "\n", 
    "Residual standard error = ", summary(model2)$sigma)
```

```{r center, fig.align = "center", out.width = "90%", fig.cap = 'Compare Model 1 (with intercept at 0) to Model 2 (with intercept at 1896).', echo=FALSE, warning=FALSE, message=FALSE}
ggplot(derby.df, aes(x = year, y = speed)) +
  geom_point() + xlim(0,2020) + ylim(0,55) +
  geom_smooth(method = lm, se = FALSE, fullrange = TRUE) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_vline(xintercept = 1896, linetype = 2) +
  geom_segment(aes(x = 100, y = 0, xend = 1800, yend = 0), 
                   arrow = arrow(length = unit(0.5, "cm")))
```

### Question 3.3 Assessing Model Adequacy

We should also attempt to verify that our LINE linear regression model assumptions fit for Model 2 if we want to make inferential statements (hypothesis tests or confidence intervals) about parameters or predictions.  Most of these assumptions can be checked graphically using a set of residual plots as in Figure \@ref(fig:resid2): 

- The upper left plot, Residuals vs. Fitted, can be used to check the Linearity assumption.  Residuals should be patternless around Y = 0; if not, there is a pattern in the data that is currently unaccounted for.
- The upper right plot, Normal Q-Q, can be used to check the Normality assumption.  Deviations from a straight line indicate that the distribution of residuals does not conform to a theoretical normal curve.
- The lower left plot, Scale-Location, can be used to check the Equal Variance assumption.  Positive or negative trends across the fitted values indicate variability that is not constant.
- The lower right plot, Residuals vs. Leverage, can be used to check for influential points.  Points with high leverage (having unusual values of the predictors) and/or high absolute residuals can have an undue influence on estimates of model parameters.  

```{r resid2, fig.align = "center", out.width = "90%", fig.cap = 'Residual plots for Model 2.', echo=FALSE, warning=FALSE}
# Residual diagnostics for Model 2
par(mfrow=c(2,2))
plot(model2)
par(mfrow=c(1,1))
plot(model2, 6)
#QQ Residuals - overall trend is linear (data is normal)
#Residual vs leverage - labeled are far away 
```

### Question 3.4 Include a Quadratic Term

Based on residual diagnostics, we should test Model 2Q, in which a quadratic term is added to the linear term in Model 2.  

\begin{equation*}
Y_{i}=\beta_{0}+\beta_{1}\textrm{Yearnew}_{i}+\beta_{2}\textrm{Yearnew}^2_{i}+\epsilon_{i}\quad \textrm{where}\quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2).
\end{equation*}

This model could suggest, for example, that the rate of increase in winning speeds is slowing down over time.  In fact, there is evidence that the quadratic model improves upon the linear model (see Figure \@ref(fig:models2and2q)).  $R^2$, \index{R-squared} the proportion of year-to-year variability in winning speeds explained by the model, has increased from 51.3\% to 64.1\%, and the pattern in the Residuals vs. Fitted plot of Figure \@ref(fig:resid2) has disappeared in Figure \@ref(fig:resid2q), although normality is a little sketchier in the left tail, and the larger mass of points with fitted values near 54 appears to have slightly lower variability.  The significantly negative coefficient for $\beta_{2}$ suggests that the rate of increase is indeed slowing in more recent years.  

```{r models2and2q, fig.align = "center", out.width = "90%", fig.cap = 'Linear (solid) vs. quadratic (dashed) fit.', echo=FALSE, warning=FALSE}
# Fitted models for Model 2 and Model 2Q
ggplot(derby.df, aes(x = year, y = speed)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x, 
              se = FALSE, linetype = 1) +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), 
              se = FALSE, linetype = 2)
```

```{r model2Q, comment=NA}
derby.df <- mutate(derby.df, yearnew2 = yearnew^2)
model2q <- lm(speed ~ yearnew + yearnew2, data = derby.df)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model2q))
cat(" R squared = ", summary(model2q)$r.squared, "\n", 
    "Residual standard error = ", summary(model2q)$sigma)
```

```{r resid2q, fig.align = "center", out.width = "90%", fig.cap = 'Residual plots for Model 2Q.', echo=FALSE, warning=FALSE}
# Residual diagnostics for Model 2
par(mfrow=c(2,2))
plot(model2q)
par(mfrow=c(1,1))
plot(model2q,1)
plot(model2q,2)
plot(model2q,3)
plot(model2q,4)
```

### Question 3.5: Linear Regression with a Binary Predictor

We also may want to include track condition as an explanatory variable.  We could start by using `fast` as the lone predictor: Do winning speeds differ for fast and non-fast conditions?  `fast` is considered an **indicator variable**---it takes on only the values 0 and 1, \index{indicator variable} where 1 indicates presence of a certain attribute (like fast racing conditions).  Since `fast` is numeric, we can use simple linear regression techniques to fit Model 3:

\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textrm{Fast}_{i}+\epsilon_{i}\quad \textrm{where}\quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2).
(\#eq:model3)
\end{equation}

Here, it's easy to see the meaning of our slope and intercept by writing out separate equations for the two conditions:

- Good or slow conditions (`fast` = 0)

\begin{equation*}
Y_{i} = \beta_{0}+\epsilon_{i}
\end{equation*}

- Fast conditions (`fast` = 1)

\begin{equation*}
Y_{i} = (\beta_{0}+\beta_{1})+\epsilon_{i}
\end{equation*}

$\beta_{0}$ is the expected winning speed under good or slow conditions, while $\beta_{1}$ is the difference between expected winning speeds under fast conditions vs. non-fast conditions.  According to our fitted Model 3, the estimated winning speed under non-fast conditions is 52.0 ft/s, while mean winning speeds under fast conditions are estimated to be 1.6 ft/s higher. 

```{r model3, comment=NA}
model3 <- lm(speed ~ fast, data = derby.df)
```

```{r}
attach(derby.df)
mean(speed[fast==1])
mean(speed[fast==0])
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model3))
cat(" R squared = ", summary(model3)$r.squared, "\n", 
    "Residual standard error = ", summary(model3)$sigma)
```

### Question 3.6: Multiple Linear Regression with Two Predictors

The beauty of the linear regression framework is that we can add explanatory variables in order to explain more variability in our response, obtain better and more precise predictions, and control for certain covariates while evaluating the effect of others.  For example, we could consider adding `yearnew` to Model 3, which has the indicator variable `fast` as its only predictor.  In this way, we would estimate the difference between winning speeds under fast and non-fast conditions *after accounting for the effect of time*.  As we observed in Figure \@ref(fig:bivariate), recent years have tended to have more races under fast conditions, so Model 3 might overstate the effect of fast conditions because winning speeds have also increased over time.  A model with terms for both year and track condition will estimate the difference between winning speeds under fast and non-fast conditions *for a fixed year*; for example, if it had rained in 2016 and turned the track muddy, how much would we have expected the winning speed to decrease?  

Our new model (Model 4) can be written:

\begin{equation}
Y_{i}=\beta_{0}+\beta_{1}\textrm{Yearnew}_{i}+\beta_{2}\textrm{Fast}_{i}+\epsilon_{i}\quad \textrm{where}\quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2).
(\#eq:model4)
\end{equation}

and linear least squares regression (LLSR) provides the following parameter estimates:

```{r model4, comment=NA}
model4 <- lm(speed ~ yearnew + fast, data = derby.df)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model4))
cat(" R squared = ", summary(model4)$r.squared, "\n", 
    "Residual standard error = ", summary(model4)$sigma)
```


### Question 3.6: Inference in Multiple Linear Regression: Normal Theory {#multreg-inference}

So far we have been using linear regression for descriptive purposes, which is an important task.  We are often interested in issues of statistical inference \index{inference} as well---determining if effects are statistically significant, quantifying uncertainty in effect size estimates with confidence intervals, and quantifying uncertainty in model predictions with prediction intervals.  Under LINE assumptions, all of these inferential tasks can be completed with the help of the t-distribution and estimated standard errors.  

Here are examples of inferential statements based on Model 4:

- We can be 95\% confident that average winning speeds under fast conditions are between 0.93 and 1.53 ft/s higher than under non-fast conditions, after accounting for the effect of year. 
- Fast conditions lead to significantly faster winning speeds than non-fast conditions (t = 8.14 on 119 df, p < .001), holding year constant.
- Based on our model, we can be 95\% confident that the winning speed in 2017 under fast conditions will be between 53.4 and 56.3 ft/s.  Note that Always Dreaming's actual winning speed barely fit within this interval---the 2017 winning speed was a borderline outlier on the slow side.  

```{r model4inf, comment=NA}
confint(model4)
new.data <- data.frame(yearnew = 2017 - 1896, fast = 1) 
predict(model4, new = new.data, interval = "prediction")
```


### Question 3.7: Multiple Linear Regression with an Interaction Term

Adding terms to form a multiple linear regression model as we did in Model 4 is a very powerful modeling tool, allowing us to account for multiple sources of uncertainty and to obtain more precise estimates of effect sizes after accounting for the effect of important covariates.  One limitation of Model 4, however, is that we must assume that the effect of track condition has been the same for 122 years, or conversely that the yearly improvements in winning speeds are identical for all track conditions.  To expand our modeling capabilities to allow the effect of one predictor to change depending on levels of a second predictor, we need to consider **interaction terms**. \index{interaction}  Amazingly, if we create a new variable by taking the product of `yearnew` and `fast` (i.e., the **interaction** between `yearnew` and `fast`), adding that variable into our model will have the desired effect.  

Thus, consider Model 5:

\begin{equation*}
\begin{split}
Y_{i}&= \beta_{0}+\beta_{1}\textrm{Yearnew}_{i}+\beta_{2}\textrm{Fast}_{i} \\
      &{}+\beta_{3}\textrm{Yearnew}_{i}\times\textrm{Fast}_{i}+\epsilon_{i}\quad \textrm{where}\quad \epsilon_{i}\sim \textrm{N}(0,\sigma^2)
\end{split}
\end{equation*}

where LLSR provides the following parameter estimates:

```{r model5, comment=NA}
model5 <- lm(speed ~ yearnew + fast + yearnew:fast, 
             data=derby.df)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model5))
cat(" R squared = ", summary(model5)$r.squared, "\n", 
    "Residual standard error = ", summary(model5)$sigma)
```

According to our model, estimated winning speeds can be found by:

\begin{equation}
 \hat{Y}_{i}=50.53+0.031\textrm{Yearnew}_{i}+1.83\textrm{Fast}_{i}-0.011\textrm{Yearnew}_{i}\times\textrm{Fast}_{i}.
(\#eq:model5est)
\end{equation}

Interpretations of model coefficients are most easily seen by writing out separate equations for fast and non-fast track conditions:

\begin{align*}
 \textrm{Fast}=0: & \\
 \hat{Y}_{i} &= 50.53+0.031\textrm{Yearnew}_{i} \\
 \textrm{Fast}=1: & \\
 \hat{Y}_{i} &= (50.53+1.83)+(0.031-0.011)\textrm{Yearnew}_{i}
 \end{align*}
 
